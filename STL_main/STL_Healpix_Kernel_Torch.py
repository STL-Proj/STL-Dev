#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HEALPix kernel-based data class for STL.

Analogue of FSTL2DKernel, but:
  - data live on HEALPix pixels (..., Npix)
  - convolutions and downsampling are performed with SphericalStencil.

Assumptions
-----------
- Data are real (or complex) PyTorch tensors.
- Last dimension is always the pixel axis.
- Pixel indexing is HEALPix (NESTED or RING, consistent with SphericalStencil.nest).
"""

import numpy as np
import torch
import torch.nn.functional as F
from SphericalStencil import SphericalStencil  # adapt if needed


###############################################################################
class STL_Healpix_Kernel_Torch:
    """HealpixKernel_torch
    HEALPix analogue of STL2DKernel.

    Attributes
    ----------
    DT : str
        Data type identifier ("").
    MR : bool
        If True, stores a list of arrays at multiple resolutions.
    N0 : int
        Initial HEALPix nside at dg=0.
    dg : int or None
        downgrading level (nside = N0 // 2**dg) if MR == False.
    list_dg : list[int] or None
        List of downgrading levels if MR == True.
    array : torch.Tensor or list[torch.Tensor]
        Data array(s):
          - MR == False : tensor of shape (..., Npix)
          - MR == True  : list of tensors with same leading dims, different Npix.
    cell_ids : torch.LongTensor or list[torch.LongTensor]
        Pixel indices corresponding to the last axis of array.
    device : torch.device
        Default device for internal tensors.
    dtype : torch.dtype
        Default dtype for internal tensors.
    """

    ###########################################################################
    def __init__(self, array, nside=None, cell_ids=None, nest=True):
        """
        Constructor for single-resolution Healpix data (MR == False).

        Parameters
        ----------
        array : np.ndarray or torch.Tensor
            Input data of shape (..., Npix).
        nside : int
            HEALPix resolution at dg=0.
        cell_ids : array-like or None
            HEALPix pixel indices for the last dimension.
            If None, assume full sky with standard ordering [0..Npix-1].
        dg : int or None
            Current downgrading level (default 0).
        nest : bool
            Whether pixel indexing is NESTED (must be consistent with SphericalStencil).
        """
        if isinstance(array, list):
            raise ValueError(
                "Only single-resolution array is accepted at construction."
            )

        # Basic metadata
        self.DT = "HealpixKernel_torch"
        self.MR = False
        self.nest = bool(nest)

        self.dg = 0

        # Store N0 as the "reference" resolution at dg=0
        if nside is None:
            nside = int(np.sqrt(array.shape[-1] // 12))

        self.N0 = [int(nside)]
        # Current nside is N0 // 2**dg
        self.nside = self.N0[0] // (2**self.dg)

        # Convert array to tensor and determine device/dtype
        self.array = self.to_array(array)
        self.device = self.array.device
        self.dtype = self.array.dtype

        # Last dimension = Npix
        Npix = self.array.shape[-1]

        # Cell ids
        if cell_ids is None:
            # Assume full-sky coverage [0..Npix-1]
            self.cell_ids = torch.arange(Npix, device=self.device, dtype=torch.long)
        else:
            self.cell_ids = self._to_cell_ids_tensor(cell_ids, Npix)

        # Multi-resolution attributes
        self.list_dg = None

    ###########################################################################
    @staticmethod
    def _default_device():
        """Return a default device (cuda if available, else cpu)."""
        return torch.device("cuda" if torch.cuda.is_available() else "cpu")

    ###########################################################################
    def _to_cell_ids_tensor(self, cell_ids, Npix_expected=None):
        """
        Convert any cell_ids-like (list/np/tensor) to a 1D LongTensor on self.device.
        Optionally check that its length matches Npix_expected.
        """
        if isinstance(cell_ids, torch.Tensor):
            cid = cell_ids.to(device=self.device, dtype=torch.long).view(-1)
        else:
            cid = torch.as_tensor(cell_ids, device=self.device, dtype=torch.long).view(
                -1
            )

        if (Npix_expected is not None) and (cid.numel() != Npix_expected):
            raise ValueError(
                f"cell_ids length {cid.numel()} does not match Npix={Npix_expected}."
            )
        return cid

    ###########################################################################
    def to_array(self, array):
        """
        Transform input array (NumPy or PyTorch) into a PyTorch tensor.

        Parameters
        ----------
        array : np.ndarray or torch.Tensor
            Input array to be converted (shape [..., Npix]).

        Returns
        -------
        torch.Tensor
            Converted PyTorch tensor on GPU if available, else CPU.
        """
        if array is None:
            return None
        elif isinstance(array, list):
            return array

        device = self._default_device()

        if isinstance(array, np.ndarray):
            return torch.from_numpy(array).to(device)
        elif isinstance(array, torch.Tensor):
            return array.to(device)
        else:
            raise TypeError(f"Unsupported array type: {type(array)}")

    ###########################################################################
    def copy(self, empty=False):
        """
        Copy a STL_Healpix_Kernel_Torch instance.

        Parameters
        ----------
        empty : bool
            If True, set array to None and cell_ids to None.

        Returns
        -------
        STL_Healpix_Kernel_Torch
            Shallow copy of metadata + (optionally) arrays.
        """
        new = object.__new__(STL_Healpix_Kernel_Torch)

        # Copy metadata
        new.DT = self.DT
        new.MR = self.MR
        new.nest = self.nest
        new.N0 = self.N0
        new.nside = self.nside
        new.dg = self.dg
        new.list_dg = list(self.list_dg) if self.list_dg is not None else None
        new.device = self.device
        new.dtype = self.dtype

        # Copy data
        if empty:
            new.array = None
            new.cell_ids = None
        else:
            if self.MR:
                new.array = [
                    a.clone() if isinstance(a, torch.Tensor) else None
                    for a in self.array
                ]
                new.cell_ids = [
                    cid.clone() if isinstance(cid, torch.Tensor) else None
                    for cid in self.cell_ids
                ]
            else:
                new.array = (
                    self.array.clone() if isinstance(self.array, torch.Tensor) else None
                )
                new.cell_ids = (
                    self.cell_ids.clone()
                    if isinstance(self.cell_ids, torch.Tensor)
                    else None
                )

        return new

    ###########################################################################
    def __getitem__(self, key):
        """
        Slice directly into the stored array(s). Only slices over leading dims;
        pixel axis is always kept.
        """
        new = self.copy(empty=True)

        if self.MR:
            if not isinstance(self.array, list):
                raise ValueError("MR=True but array is not a list.")

            # Slice each resolution with the same key
            new.array = [a[key] for a in self.array]
            new.cell_ids = self.cell_ids[:]  # same pixel ids per resolution
        else:
            new.array = self.array[key]
            new.cell_ids = self.cell_ids

        return new

    ###########################################################################
    def modulus(self, inplace=False):
        """
        Compute the modulus (absolute value) of the data.

        For complex data, uses torch.abs. For real data, it is just |x|.
        """
        data = self.copy(empty=False) if not inplace else self

        if data.MR:
            data.array = [torch.abs(a) for a in data.array]
        else:
            data.array = torch.abs(data.array)

        data.dtype = data.array[0].dtype if data.MR else data.array.dtype
        return data

    ###########################################################################
    def mean(self, square=False):
        """
        Compute the mean over the last (pixel) dimension.

        Parameters
        ----------
        square : bool
            If True, use |x|^2 instead of x.

        Returns
        -------
        torch.Tensor
            Mean values over pixels. If MR=True, the last dimension is len(list_dg).
        """
        if self.MR:
            means = []
            for arr in self.array:
                arr_use = torch.abs(arr) ** 2 if square else arr
                means.append(arr_use.nanmean(dim=-1))
            # Stack along a new last dimension corresponding to list_dg
            return torch.stack(means, dim=-1)
        else:
            if self.array is None:
                raise ValueError("No data stored in this object.")
            arr_use = torch.abs(self.array) ** 2 if square else self.array
            return arr_use.nanmean(dim=-1)

    ###########################################################################
    def cov(self, data2=None, remove_mean=False):
        """
        Compute covariance along the pixel axis between self and data2.

        Only supports MR == False.

        Parameters
        ----------
        data2 : STL_Healpix_Kernel_Torch or None
            If None, compute auto-covariance of self.
        remove_mean : bool
            If True, subtract the mean before multiplying.

        Returns
        -------
        torch.Tensor
            Covariance values over the last dimension.
        """
        if self.MR:
            raise ValueError("cov currently supports only MR == False.")

        x = self.array
        if data2 is None:
            y = x
        else:
            if not isinstance(data2, STL_Healpix_Kernel_Torch):
                raise TypeError("data2 must be a STL_Healpix_Kernel_Torch instance.")
            if data2.MR:
                raise ValueError("data2 must have MR == False.")
            if data2.dg != self.dg:
                raise ValueError("data2 must have the same dg as self.")
            y = data2.array

        dim = -1  # pixel axis

        if remove_mean:
            mx = x.mean(dim=dim, keepdim=True)
            my = y.mean(dim=dim, keepdim=True)
            x_c = x - mx
            y_c = y - my
        else:
            x_c = x
            y_c = y

        cov = (x_c * y_c.conj()).nanmean(dim=dim)
        return cov

    ###########################################################################
    def get_wavelet_op(self, kernel_size=None, L=None, J=None):
        """
        Build a Healpix wavelet operator, analogous to get_wavelet_op() in STL2DKernel.
        """
        if L is None:
            L = 4
        if kernel_size is None:
            kernel_size = 5
        if J is None:
            J = int(np.log2(self.N0))

        return WavelateOperatorHealpixKernel_torch(
            kernel_size=kernel_size,
            nside=self.N0[0],
            L=L,
            J=J,
            device=self.device,
            dtype=self.dtype,
        )


###############################################################################
class WavelateOperatorHealpixKernel_torch:
    """
    Healpix wavelet operator using SphericalStencil.

    - Build a directional wavelet kernel on a local KxK stencil (tangent plane).
    - Flatten to shape (Ci=1, L, P=K^2).
    - Use SphericalStencil.Convol_torch to convolve maps.

    For now we implement a simple "Morlet-like" directional kernel similar
    to WavelateOperator2Dkernel_torch in STL2DKernel.
    """

    def __init__(
        self,
        kernel_size: int,
        nside: int,
        L: int,
        J: int,
        device="cuda",
        dtype=torch.float,
    ):

        # Reuse cached stencils for this resolution / kernel
        self.KERNELSZ = kernel_size
        self.L = L
        self.J = J
        self.device = torch.device(device)
        self.dtype = dtype
        self.WType = "HealpixWavelet"
        self.nest = True
        self.nside = nside

        # Build (1, L, P) kernel, where P=K^2
        kernel_2d = self._wavelet_kernel(kernel_size, L)  # (1, L, K, K)
        self.kernel = kernel_2d.reshape(
            1, 1, kernel_size * kernel_size
        )  # (Ci=1, Co=L, P)
        # smooth kernel coded
        # x,y=np.meshgrid(np.arange(5)-2,np.arange(5)-2)
        # sigma=1.0
        # np.exp(-(x**2+y**2)/(2*sigma**2))
        self.smooth_kernel = self.kernel.abs().reshape(1, 1, kernel_size, kernel_size)
        self.smooth_kernel = self.smooth_kernel / self.smooth_kernel.sum()

    def _wavelet_kernel(self, kernel_size: int, n_orientation: int, sigma=1.0):
        """
        Create a 2D directional wavelet kernel on a KxK grid, similar to
        WavelateOperator2Dkernel_torch._wavelet_kernel.

        Returns
        -------
        kernel : torch.Tensor
            Complex tensor of shape (1, n_orientation, K, K).
        """
        coords = (
            torch.arange(kernel_size, device=self.device, dtype=self.dtype)
            - (kernel_size - 1) / 2.0
        )
        yy, xx = torch.meshgrid(coords, coords, indexing="ij")

        # Isotropic Gaussian envelope
        mother_kernel = torch.exp(-(xx**2 + yy**2) / (2 * sigma**2))[None, :, :]

        # Orientations done in the gauges paradigm
        angles_proj = 0.5 * torch.pi * (xx[None, ...])

        kernel = torch.complex(
            torch.cos(angles_proj) * mother_kernel,
            torch.sin(angles_proj) * mother_kernel,
        )
        # --- cache for SphericalStencil objects (one per resolution / kernel config) ---
        # key: (dg, kernel_sz, n_gauges, gauge_type, nest)
        self._stencil_cache = {}

        # Zero-mean and normalization per orientation
        kernel = kernel - torch.mean(kernel, dim=(1, 2), keepdim=True)
        kernel = kernel / torch.sum(kernel.abs(), dim=(1, 2), keepdim=True)

        return kernel.reshape(1, 1, kernel_size, kernel_size)

    ###########################################################################
    def _get_stencil(
        self,
        dg: int,
        cell_ids,
        kernel_sz: int,
        n_gauges: int = 1,
        gauge_type: str = "cosmo",
    ):
        """
        Return a cached SphericalStencil for the current resolution (dg, nside)
        and given kernel_sz / n_gauges / gauge_type.

        If it doesn't exist yet, build it once and store in self._stencil_cache.
        """
        # Key identifies geometry: dg controls nside and Npix, and nest is important
        key = (int(dg), int(kernel_sz), int(n_gauges), str(gauge_type), bool(self.nest))

        stencil = self._stencil_cache.get(key, None)
        if stencil is None:
            # Build it once
            cid_np = cell_ids.detach().cpu().numpy().astype(np.int64)
            stencil = SphericalStencil(
                nside=self.nside // (2**dg),
                kernel_sz=kernel_sz,
                nest=self.nest,
                cell_ids=cid_np,
                n_gauges=n_gauges,
                gauge_type=gauge_type,
                device=self.device,
                dtype=self.dtype,
            )
            self._stencil_cache[key] = stencil
        else:
            # Rebind device/dtype if they've changed (geometry stays valid)
            stencil.device = self.device
            stencil.dtype = self.dtype

        return stencil

    def get_L(self):
        return self.L

    def apply(self, data: STL_Healpix_Kernel_Torch, j: int):
        """
        Apply the wavelet convolution to a STL_Healpix_Kernel_Torch instance.

        Parameters
        ----------
        data : STL_Healpix_Kernel_Torch
            Input Healpix data with array of shape [..., K] and cell_ids aligned.
            Must be at downgrading level dg == j.
        j : int
            Scale index. We simply check consistency with data.dg.

        Returns
        -------
        STL_Healpix_Kernel_Torch
            New object with array shape [..., L, K], same nside & cell_ids.
        """
        if j != data.dg:
            raise ValueError(
                "j is not equal to data.dg; convolution not consistent with scale."
            )

        x = data.array  # [..., K]
        cid = data.cell_ids
        *leading, K = x.shape

        # Flatten leading dims into batch dimension: (B, Ci=1, K)
        if leading:
            B = int(np.prod(leading))
        else:
            B = 1
        x_bc = x.reshape(B, 1, K)

        # Kernel for SphericalStencil: (Ci=1, Co=L, P)
        wr = torch.real(self.kernel).to(device=data.device, dtype=data.dtype)
        wi = torch.imag(self.kernel).to(device=data.device, dtype=data.dtype)

        l_stencil = self._get_stencil(data.dg, cid, self.KERNELSZ, n_gauges=self.L)

        # Use the same stencil but rebind device/dtype if needed
        if (l_stencil.device != data.device) or (l_stencil.dtype != data.dtype):
            # No heavy re-init: we just update device/dtype (geometry is cached in stencil)
            l_stencil.device = data.device
            l_stencil.dtype = data.dtype

        # Convolution on sphere -> (B, L, K)
        y_bc = torch.complex(
            l_stencil.Convol_torch(x_bc, wr, cell_ids=cid.detach().cpu().numpy()),
            l_stencil.Convol_torch(x_bc, wi, cell_ids=cid.detach().cpu().numpy()),
        )
        if not isinstance(y_bc, torch.Tensor):
            y_bc = torch.as_tensor(y_bc, device=data.device, dtype=data.dtype)

        _, L, K_out = y_bc.shape
        y = y_bc.reshape(*leading, L, K_out)  # [..., L, K]

        # Wrap into a new STL_Healpix_Kernel_Torch (same nside, same cell_ids, same dg)
        out = data.copy(empty=True)
        out.MR = False
        out.array = y
        out.cell_ids = cid.clone()
        out.dg = data.dg
        out.nside = data.nside
        out.N0 = data.N0
        out.list_dg = None
        return out

    def apply_smooth(self, data: STL_Healpix_Kernel_Torch, inplace: bool = True):
        """
        Smooth the data by convolving with a smooth kernel derived from the
        wavelet orientation 0. The data shape is preserved.

        Parameters
        ----------
        data : STL_Healpix_Kernel_Torch
            Input Healpix data with array of shape [..., K] and cell_ids aligned.
        copy : bool
            If True, return a new STL_Healpix_Kernel_Torch instance.
            If False, modify the input object in-place and return it.

        Returns
        -------
        STL_Healpix_Kernel_Torch
            Smoothed data object with same shape as input (no extra L dimension).
        """
        x = data.array  # [..., K]
        cid = data.cell_ids
        *leading, K = x.shape

        # Flatten leading dims into batch dimension: (B, Ci=1, K)
        if leading:
            B = int(np.prod(leading))
        else:
            B = 1
        x_bc = x.reshape(B, 1, K)

        # Smooth kernel (Ci=1, Co=1, P)
        w_smooth = self.kernel.abs().to(device=data.device, dtype=data.dtype)

        l_stencil = self._get_stencil(data.dg, cid, self.KERNELSZ, n_gauges=1)
        # Make sure stencil uses the right device/dtype
        if (l_stencil.device != data.device) or (l_stencil.dtype != data.dtype):
            l_stencil.device = data.device
            l_stencil.dtype = data.dtype

        # Convolution on sphere -> (B, 1, K)
        y_bc = l_stencil.Convol_torch(
            x_bc, w_smooth, cell_ids=cid.detach().cpu().numpy()
        )

        if not isinstance(y_bc, torch.Tensor):
            y_bc = torch.as_tensor(y_bc, device=data.device, dtype=data.dtype)

        y = y_bc.reshape(*leading, K)  # same shape as input x

        # Copy or in-place update
        out = data.copy(empty=True) if not inplace else data
        out.array = y
        # metadata stays identical (nside, N0, dg, cell_ids, ...)
        return out

    def _smooth_with_nan(self, data: STL_Healpix_Kernel_Torch, inplace: bool = True):
        """
        Smooth the data by convolving with a smooth kernel derived from the
        wavelet kernel, handling NaNs:

          - NaNs are treated as missing values.
          - We convolve both the data (with NaNs replaced by 0) and a mask
            of valid pixels.
          - The result is num / w_sum, i.e. normalized by the sum of weights
            of valid pixels (nan-aware smoothing).

        Parameters
        ----------
        data : STL_Healpix_Kernel_Torch
            Input Healpix data with array of shape [..., K] and cell_ids aligned.
        inplace : bool
            If True, modify `data` in-place. Otherwise, work on a copy.

        Returns
        -------
        STL_Healpix_Kernel_Torch
            Smoothed data object with same shape as input.
        """
        x = data.array  # [..., K]
        cid = data.cell_ids
        *leading, K = x.shape

        # Work on a copy or not
        out = data if inplace else data.copy(empty=False)

        # Flatten leading dims into (B, 1, K)
        if leading:
            B = int(np.prod(leading))
        else:
            B = 1
        x_bc = x.reshape(B, 1, K)  # (B, 1, K)

        # Build mask of valid pixels (1 for valid, 0 for NaN)
        mask_valid = ~torch.isnan(x_bc)  # (B,1,K) bool
        mask_f = mask_valid.to(x_bc.dtype)  # float
        # Replace NaN by 0 so they do not contribute
        x_filled = torch.where(mask_valid, x_bc, torch.zeros_like(x_bc))

        # Smooth kernel (Ci=1, Co=1, P)
        # Here we take magnitude of the wavelet kernel and normalize it
        w_smooth = self.kernel.abs().to(device=data.device, dtype=data.dtype)  # (1,1,P)
        w_smooth = w_smooth / (w_smooth.sum(dim=-1, keepdim=True) + 1e-12)

        stencil = self._get_stencil(data.dg, cid, self.KERNELSZ, n_gauges=1)
        if (stencil.device != data.device) or (stencil.dtype != data.dtype):
            stencil.device = data.device
            stencil.dtype = data.dtype

        cid_np = cid.detach().cpu().numpy().astype(np.int64)

        # Convolution on sphere for data and mask
        num = stencil.Convol_torch(x_filled, w_smooth, cell_ids=cid_np)  # (B,1,K)
        w_sum = stencil.Convol_torch(mask_f, w_smooth, cell_ids=cid_np)  # (B,1,K)

        if not isinstance(num, torch.Tensor):
            num = torch.as_tensor(num, device=data.device, dtype=data.dtype)
        if not isinstance(w_sum, torch.Tensor):
            w_sum = torch.as_tensor(w_sum, device=data.device, dtype=data.dtype)

        eps = 1e-8
        y_bc = num / (w_sum + eps)  # (B,1,K)
        y_bc = y_bc.reshape(*leading, K)  # [..., K]

        # Pixels with no valid neighbors in the smoothing kernel -> NaN
        no_valid = (w_sum <= 0).reshape(*leading, K)
        y_bc = torch.where(no_valid, torch.full_like(y_bc, float("nan")), y_bc)

        out.array = y_bc
        return out

    ###########################################################################
    def downsample(
        self,
        data: STL_Healpix_Kernel_Torch,
        dg_out: int,
        inplace: bool = True,
        smooth: bool = True,
    ):
        """
        Downsample the data to a coarser dg_out level in one step, using NESTED
        binning on HEALPix indices.

        Logic:
          - Optionally smooth the map at current resolution to remove small scales.
          - Group pixels by their parent index in NESTED scheme:
                parent_id = cell_id // 4**Δg
            where Δg = dg_out - dg.
          - For each parent pixel, average all children pixels' values.

        Only supports MR == False.

        Parameters
        ----------
        dg_out : int
            Target downgrading level (dg_out >= self.dg >= 0).
        inplace : bool
            If True, modify the current object.
            If False, return a new object and leave self unchanged.
        smooth : bool
            If True, apply smoothing before binning.

        Returns
        -------
        STL_Healpix_Kernel_Torch
            Data at the desired dg_out resolution.
        """
        if data.MR:
            raise ValueError("downsample only supports MR == False.")

        dg_out = int(dg_out)
        if dg_out < 0:
            raise ValueError("dg_out must be non-negative.")

        if dg_out < data.dg:
            raise ValueError("Requested dg_out < current dg; upsampling not supported.")

        # Trivial case
        if dg_out == data.dg:
            return data if inplace else data.copy(empty=False)

        # Work on a copy or in-place
        data = data if inplace else data.copy(empty=False)

        # 1) Smoothing step (optional)
        if smooth:
            data = self.apply_smooth(data, inplace=True)

        # 2) Binning in NESTED scheme
        delta_g = dg_out - data.dg
        factor_pix = 4**delta_g  # 4^Δg children per parent in NESTED

        cid = data.cell_ids  # (K,)
        # Parent indices in NESTED
        parent_ids = cid // factor_pix  # (K,)

        # We want to average data.array[..., K] over these parent_ids
        x = data.array
        *leading, K = x.shape
        if leading:
            B = int(np.prod(leading))
        else:
            B = 1

        # Flatten leading dims into batch dimension: (B, K)
        x_flat = x.reshape(B, K)

        # Unique parent indices and inverse mapping
        parent_unique, inv = torch.unique(parent_ids, return_inverse=True)
        Kc = parent_unique.numel()  # number of coarse pixels

        # Scatter-add sums into coarse bins
        out = torch.zeros(B, Kc, device=x.device, dtype=x.dtype)
        idx = inv.unsqueeze(0).expand(B, -1)  # (B, K)
        out.scatter_add_(1, idx, x_flat)

        # Compute counts per coarse pixel (non-zero by construction)
        counts = torch.bincount(inv, minlength=Kc).to(x.dtype)  # (Kc,)
        out = out / counts.unsqueeze(0)  # broadcast over batch

        # Reshape back to original leading dims + coarse pixel axis
        y = out.reshape(*leading, Kc)

        # Update data object
        data.array = y
        data.cell_ids = parent_unique.to(device=data.device, dtype=torch.long)
        data.dg = dg_out
        # New nside from N0 and dg_out (conceptually underlying grid)
        data.nside = data.N0[0] // (2**dg_out)

        return data

    ###########################################################################
    def nandownsample(
        self,
        data: STL_Healpix_Kernel_Torch,
        dg_out: int,
        inplace: bool = True,
        smooth: bool = True,
    ):
        """
        Nan-aware downsampling of Healpix data to a coarser dg_out level,
        using NESTED binning and nanmean:

        Logic
        -----
        - Optionally smooth the map at current resolution with nan-aware
          smoothing (see _smooth_with_nan) to remove small scales.
        - Group pixels by their parent index in NESTED scheme:
              parent_id = cell_id // 4**Δg
          where Δg = dg_out - dg.
        - For each parent pixel and each batch element, compute the nanmean
          of children values:
              sum(children_valid) / count(children_valid).

        Only supports MR == False.

        Parameters
        ----------
        data : STL_Healpix_Kernel_Torch
            Input data at resolution dg.
        dg_out : int
            Target downgrading level (dg_out >= data.dg >= 0).
        inplace : bool
            If True, modify `data` in-place.
            If False, return a new STL_Healpix_Kernel_Torch.
        smooth : bool
            If True, apply nan-aware smoothing before binning.

        Returns
        -------
        STL_Healpix_Kernel_Torch
            Data at the desired dg_out resolution.
        """
        if data.MR:
            raise ValueError("nandownsample only supports MR == False.")

        dg_out = int(dg_out)
        if dg_out < 0:
            raise ValueError("dg_out must be non-negative.")
        if dg_out < data.dg:
            raise ValueError("Requested dg_out < current dg; upsampling not supported.")

        # Trivial case
        if dg_out == data.dg:
            return data if inplace else data.copy(empty=False)

        # Work on a copy or in-place
        data = data if inplace else data.copy(empty=False)

        # 1) Optional nan-aware smoothing
        if smooth:
            data = self._smooth_with_nan(data, inplace=True)

        # 2) Binning in NESTED scheme with nanmean
        delta_g = dg_out - data.dg
        factor_pix = 4**delta_g  # 4^Δg children per parent in NESTED

        cid = data.cell_ids  # (K,)
        parent_ids = cid // factor_pix  # (K,)

        x = data.array  # [..., K]
        *leading, K = x.shape
        if leading:
            B = int(np.prod(leading))
        else:
            B = 1

        # Flatten leading dims into batch dimension: (B, K)
        x_flat = x.reshape(B, K)

        # Handle NaNs: build mask of valid children
        mask_valid = ~torch.isnan(x_flat)
        mask_f = mask_valid.to(x_flat.dtype)
        x_filled = torch.where(mask_valid, x_flat, torch.zeros_like(x_flat))

        # Unique parent indices and inverse mapping from children -> parent
        parent_unique, inv = torch.unique(parent_ids, return_inverse=True)
        Kc = parent_unique.numel()  # number of coarse pixels

        # Scatter-add sums into coarse bins (per batch)
        idx = inv.unsqueeze(0).expand(B, -1)  # (B, K)

        out_sum = torch.zeros(B, Kc, device=x_flat.device, dtype=x_flat.dtype)
        out_sum.scatter_add_(1, idx, x_filled)

        out_count = torch.zeros(B, Kc, device=x_flat.device, dtype=x_flat.dtype)
        out_count.scatter_add_(1, idx, mask_f)

        eps = 1e-8
        out = out_sum / (out_count + eps)  # nan-aware mean
        # No valid child for some parent in a given batch → NaN
        no_valid_parent = out_count <= 0
        out = torch.where(no_valid_parent, torch.full_like(out, float("nan")), out)

        # Reshape back to original leading dims + coarse pixel axis
        y = out.reshape(*leading, Kc)

        # Update data object
        data.array = y
        data.cell_ids = parent_unique.to(device=data.device, dtype=torch.long)
        data.dg = dg_out
        data.nside = data.N0[0] // (2**dg_out)

        return data
